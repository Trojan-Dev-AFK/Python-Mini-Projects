{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: pillow in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: torch in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (0.20.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.0-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tqdm in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp311-cp311-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading scikit_learn-1.6.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.4/11.1 MB 16.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.6/11.1 MB 15.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 13.4 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.0-cp311-cp311-win_amd64.whl (8.0 MB)\n",
      "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 3.9/8.0 MB 19.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/8.0 MB 16.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.0/8.0 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fonttools-4.55.3-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.8 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.6/25.1 MB 12.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.6/25.1 MB 16.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 9.2/25.1 MB 15.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.7/25.1 MB 11.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.6/25.1 MB 12.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.5/25.1 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.1/25.1 MB 12.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.0/25.1 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.1/25.1 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Using cached multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, threadpoolctl, scipy, pyparsing, pyarrow, propcache, multidict, kiwisolver, joblib, fsspec, frozenlist, fonttools, dill, cycler, contourpy, attrs, aiohappyeyeballs, yarl, scikit-learn, pandas, multiprocess, matplotlib, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 attrs-24.3.0 contourpy-1.3.1 cycler-0.12.1 datasets-3.2.0 dill-0.3.8 fonttools-4.55.3 frozenlist-1.5.0 fsspec-2024.9.0 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.0 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.2.1 pyarrow-18.1.0 pyparsing-3.2.1 pytz-2024.2 scikit-learn-1.6.0 scipy-1.14.1 threadpoolctl-3.5.0 tzdata-2024.2 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers pillow torch torchvision datasets scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from transformers[torch]) (2.5.1)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (6.1.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from torch->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from torch->transformers[torch]) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from requests->transformers[torch]) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\development\\python-mini-projects\\trocr_datetimestampextractor\\.venv\\lib\\site-packages (from jinja2->torch->transformers[torch]) (3.0.2)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(TorchDataset):\n",
    "    def __init__(self, dataframe, processor, image_dir):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = Compose([ToTensor(), Normalize(mean=[0.5], std=[0.5])])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.dataframe.iloc[idx]['image_path'])\n",
    "        text = self.dataframe.iloc[idx]['text']\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = self.processor.image_processor(image, return_tensors=\"pt\").pixel_values[0]\n",
    "\n",
    "        labels = self.processor.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids[0]\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(processor, train_csv, val_csv, image_dir):\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    val_df = pd.read_csv(val_csv)\n",
    "\n",
    "    train_dataset = OCRDataset(train_df, processor, image_dir)\n",
    "    val_dataset = OCRDataset(val_df, processor, image_dir)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_collator(features):\n",
    "    # Stack `pixel_values` and `labels` tensors from the batch\n",
    "    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
    "    labels = torch.stack([f[\"labels\"] for f in features])\n",
    "\n",
    "    # Return a dictionary compatible with the VisionEncoderDecoderModel\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        logging.info(f\"Inputs received in compute_loss: {inputs.keys()}\")\n",
    "        logging.info(f\"Unexpected kwargs: {kwargs}\")\n",
    "        inputs = {k: v for k, v in inputs.items() if k in [\"pixel_values\", \"labels\"]}\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(processor, model, train_dataset, val_dataset, output_dir, training_args):\n",
    "    trainer = CustomSeq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        data_collator=custom_data_collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    # Save the processor to the same directory\n",
    "    processor.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./input/TSfinetuning/\"\n",
    "train_csv = \"./training_data/train.csv\"\n",
    "val_csv = \"./training_data/validation.csv\"\n",
    "fine_tuned_model_path = \"./trained_model/\"\n",
    "output_text_file = \"./output/trained_model_results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = load_data(processor, train_csv, val_csv, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Development\\Python-Mini-Projects\\TrOCR_DateTimestampExtractor\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=fine_tuned_model_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    save_strategy=\"no\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aloysius\\AppData\\Local\\Temp\\ipykernel_17460\\4034245244.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomSeq2SeqTrainer(\n",
      "  0%|          | 0/5 [01:23<?, ?it/s]\n",
      " 20%|██        | 1/5 [00:07<00:28,  7.17s/it]\n",
      "                                             \n",
      "\u001b[A                                  \n",
      "\n",
      " 20%|██        | 1/5 [00:08<00:28,  7.17s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.040663719177246, 'eval_runtime': 1.5158, 'eval_samples_per_second': 0.66, 'eval_steps_per_second': 0.66, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:14<00:21,  7.15s/it]\n",
      "                                             \n",
      "\u001b[A                                  \n",
      "\n",
      " 40%|████      | 2/5 [00:15<00:21,  7.15s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7549625635147095, 'eval_runtime': 1.302, 'eval_samples_per_second': 0.768, 'eval_steps_per_second': 0.768, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:21<00:14,  7.06s/it]\n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      " 60%|██████    | 3/5 [00:22<00:14,  7.06s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43727830052375793, 'eval_runtime': 1.2827, 'eval_samples_per_second': 0.78, 'eval_steps_per_second': 0.78, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:28<00:06,  6.99s/it]\n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      " 80%|████████  | 4/5 [00:29<00:06,  6.99s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23852960765361786, 'eval_runtime': 1.3277, 'eval_samples_per_second': 0.753, 'eval_steps_per_second': 0.753, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:34<00:00,  6.92s/it]\n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "100%|██████████| 5/5 [00:36<00:00,  6.92s/it]\n",
      "\u001b[A\n",
      "                                             \n",
      "100%|██████████| 5/5 [00:36<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17585328221321106, 'eval_runtime': 1.1789, 'eval_samples_per_second': 0.848, 'eval_steps_per_second': 0.848, 'epoch': 5.0}\n",
      "{'train_runtime': 36.1481, 'train_samples_per_second': 0.138, 'train_steps_per_second': 0.138, 'train_loss': 5.4151161193847654, 'epoch': 5.0}\n",
      "Fine-tuning complete. Model saved!\n"
     ]
    }
   ],
   "source": [
    "fine_tune_model(processor, model, train_dataset, val_dataset, fine_tuned_model_path, training_args)\n",
    "print(\"Fine-tuning complete. Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = VisionEncoderDecoderModel.from_pretrained(fine_tuned_model_path)\n",
    "processor = TrOCRProcessor.from_pretrained(fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_fine_tuned_model(image_path, processor, model):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = processor.image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return generated_text\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {image_path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_in_folder(folder_path, output_file, processor, model):\n",
    "    results = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            recognized_text = extract_text_with_fine_tuned_model(image_path, processor, model)\n",
    "            results.append(f\"{filename}: {recognized_text}\")\n",
    "            print(f\"Processed {filename}\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(results))\n",
    "\n",
    "    print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./input/TSfinetuning/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3142640_box_1_1_0.png\n",
      "Results saved to ./output/trained_model_results.txt\n"
     ]
    }
   ],
   "source": [
    "process_images_in_folder(image_dir, output_text_file, processor, fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
